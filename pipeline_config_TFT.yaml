TrainTestPipelineConfig:
  config_name: TFT
  custom_parameters:
    scaler_name: MinMaxScaler
#***************************** MLFLOW ***********************************************************



    mlflow_experiment_name: fusion_forecast_not_serious
    run_name: "first run"     #run_name is  folder name , also run_name for mlflow
    run_description: "Please write a description of your run here"           #Please write a description of your run here, why you want to run
    my_custom_tag: checkrun_notserious
    log_every_n_step: 100    #it logs every epoch but after how many steps do you want it to log. if 1 it effects performance
    log_models: True

    is_real_prediction: False
    run_pipeline_from_dataframe: False                #runs training and prediction from a data frame

    datapreprocessor:

      save_imputed_featureengineered_df: False
      save_imputed_featureengineered_df_filename: data/after_fillingna_complete_s.pkl"

    fusionforecast:
      file_path: "data/CF_physics_Jan20,23_a.csv"

    imputemissingvalues:
      load_imputed_df: False
      load_imputed_df_name: data/after_fillingna_complete_s.pkl


    structuringdataset:
      #encoding and normalization
      categorical_encoding : nanlabelencoder
      static_numerical_scaler : none                           #(standard, minmax, robust, none), this is applied to reals
      time_varying_unknown_numerical_scaler : robust         #(standard, minmax, robust, none), this is applied to reals
      time_varying_known_numerical_scaler : none                #(standard, minmax, robust, none), this is applied to reals


      #making time series dataset from the data frame
      max_prediction_length: 10
      max_encoder_length: 5                                # how much past to look into
      min_encoder_length: 5                                 # Item_IDS which does not possess minimum encoder length are removed

      group_ids:                  # Add PRODUCTLINE as part of this list, in case we want a group and make predictions on groups
      - ITEM_ID                   # grouping might give better averages on grouping
      #- STYLE_ID                 # one among PRODUCTLINE or STYLE can be used
     # - PRODUCTLINE              # giving all the three levels to group and predict, not sure if good idea to do it
      batch_size: 32           #32 was good
      num_workers: 8         # number of workers (CPUS to use in parallel)

    predict_class:

      toread_mlflow_experiment_id: 1
      toread_mlflow_run_id: 7fbcea5277bc46c6ad7fe5734137613d
      toread_mlflow_ckpt_filename: epoch=0-step=32.ckpt
      toread_mlflow_train_filename: TFT_12-14-2022__21-29-58_train_df.pkl     #this file contains both the train and the test data




    #    trainer:
  #    batch_size: 32                             #32 was good

    tftmodeldefinition:
      #Model Parameterts
      accelerator: cpu
      devices: 1
      max_epochs: 10
      gpus: 0
      gradient_clip_val : 0.05                      # clipping gradients is a hyperparameter and important to prevent divergance
                                                 # of the gradient for recurrent neural networks
      learning_rate : 0.0001                           #very important
      hidden_size :  5                            #most important hyperparameter apart from learning rate
      attention_head_size : 1                    # number of attention heads. Set to up to 4 for large dataset
      dropout : 0.5                                 #between 0.1 and 0.3 are good values
      hidden_continuous_size: 6                   # set to <= hidden_size
      output_size: 7                             # number of quantiles to predict, the quantiles to predict are given below
                                                 #loss by default is kept as Quantileloss



      list_of_quantiles:                          #no.of Quantiles [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]
      - 0.02                                      # In industry, I remember seeing 80% quantile
      - 0.1
      - 0.25
      - 0.5                                    #Incase the average is below the expected, reduce 0.5 a bit or otherway around
      - 0.75                                   #This way we can increase the accuracy of the overall forecast
      - 0.9                                    #Even the other quantiles can be changed to get the envelopes as required
      - 0.98

      log_interval: -1                          #logging  after every log_interval number of batches
      reduce_on_plateau_patience: 4              #wait for these many epochs during training before reducing the learning rate by a factor
      reduce_on_plateau_reduction: 2.0          # lr gets divided by this number after the patience
      reduce_on_plateau_min_lr: 0.00001          #dont touch this until in special situations

      weight_decay: 0.01                       #the weight decay or L2 regularization, incase the data is getting over fit
      optimizer: "ranger"                      #choose between(ranger, sgd, adam, adamw)
      share_single_variable_networks: False    # hard or soft sharing between encoder and decoder

      early_stop_callback_monitor: val_MAE      #(val_RMSE etc... can also monitored
      early_stop_callback_min_delta: 0.0001     #min validation error change to consoder for callback
      early_stop_callback_patience: 10          # Number of epochs to wait before stopping the run, if the validation error doesnot improve

      tensorboard_log_dir : lightning_logs
      quick_debug: False






    optunatuning:
      optuna_best_model_save_dir: "optuna_first"             #prefer to give with optuna_  shall save inside lightning_logs directory
      optuna_save_study_filename: first

    tftdfcmodel:
      run_type: "train"    #choose between (train, optuna_tune, predict, find_lr)



    # Existing custom parameters which could be useful to me


    #scaler_name: MinMaxScaler
    split_by_columns:
      - REGION_ID
      - U_PRODUCTLINE
    threshold1: 10000
    threshold2: 50000

    log_DFC_results: True
    is_asa_run: False
    save_pickle_file: True
    save_model_file: True

#    drop_duplicates_if_exists: true
#    dropout_ratio: 0.5
#    epochs: 25
    filter_by_visibility: 0.01
    loss: mean_squared_error
    min_delta: 0.1
    EXPERIMENT_NAME: 1_BP_DE_U_PRODUCTLINE


    regions:
      - BP_DE
  timeframes:
    train_1: #2018 #updated as per ISO
      end_date: 30.12.2018
      historic_date: 02.01.2017
      reference_date: 31.12.2017
      start_date: 01.01.2018
      post_loading_filter:
        - 'COLL_TYP_NAME == "S"'

    train_2: # 2019
      end_date: 29.12.2019
      historic_date: 01.01.2018
      reference_date: 30.12.2018
      start_date: 31.12.2018
      post_loading_filter:
        - 'COLL_TYP_NAME == "S"'

    train_3: # 2020
      end_date: 03.01.2021
      historic_date: 31.12.2018
      reference_date: 29.12.2019
      start_date: 30.12.2019
      post_loading_filter:
        - 'COLL_TYP_NAME == "S"'

    train_4: #   2021
      end_date: 02.01.2022
      historic_date: 06.01.2020
      reference_date: 03.01.2021
      start_date: 04.01.2021
      post_loading_filter:
        - 'COLL_TYP_NAME == "S"'


    #    train_trail_test: #   2022
    #      end_date: 01.01.2023
    #      historic_date: 04.01.2021
    #      reference_date: 02.01.2022
    #      start_date: 03.01.2022
    #      post_loading_filter:
    #        - 'COLL_TYP_NAME == "S"'





    #    train_2: # 2020
    #      historic_date: 31.12.2018
    #      reference_date: 29.12.2019
    #      start_date: 30.12.2019
    #      end_date: 03.01.2021
    #    train_3: # 2021-H1
    #      historic_date: 02.01.2020
    #      reference_date: 31.12.2020
    #      start_date: 01.01.2021
    #      end_date: 30.06.2021
    #
    #    validation: # 2019-H2
    #      historic_date: 01.07.2018
    #      reference_date: 30.06.2019
    #      start_date: 01.07.2019
    #      end_date: 29.12.2019
    #
    #    test_1: # 2019-H1
    #      historic_date: 01.01.2018
    #      reference_date: 30.12.2018
    #      start_date: 31.12.2018
    #      end_date: 30.06.2019
    #    test_2: # 2021-H2
    #      historic_date: 30.06.2020
    #      reference_date: 30.06.2021
    #      start_date: 01.07.2021
    #      end_date: 31.12.2021
    #
    #    real_prediction: # 2022
    #      historic_date: 21.02.2021
    #      reference_date: 28.02.2022
    #      start_date: 01.03.2022
    #      end_date: 28.02.2023
#      real_prediction:
#        end_date: 28.02.2023
#        historic_date: 21.02.2021
#        reference_date: 28.02.2022
#        start_date: 01.03.2022
#      test_1:
#        end_date: 30.06.2019
#        historic_date: 01.01.2018
#        reference_date: 07.01.2019
#        start_date: 31.12.2018
#          test_2:
#            end_date: 31.12.2021
#            historic_date: 30.06.2020
#            reference_date: 30.06.2021
#            start_date: 01.07.2021
#      train_1:
#        end_date: 30.12.2018
#        historic_date: 02.01.2017
#        reference_date: 31.12.2017
#        start_date: 01.01.2018
#          train_2:
#            end_date: 03.01.2021
#            historic_date: 31.12.2018
#            reference_date: 29.12.2019
#            start_date: 30.12.2019
#          train_3:
#            end_date: 30.06.2021
#            historic_date: 02.01.2020
#            reference_date: 31.12.2020
#            start_date: 01.01.2021
#      validation:
#        end_date: 29.12.2019
#        historic_date: 01.07.2018
#        reference_date: 30.06.2019
#        start_date: 01.07.2019
LoggerConfig:
  log_directory: log
  log_name_prefix: traintest_
  log_level: DEBUG